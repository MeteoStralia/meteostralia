{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, CondensedNearestNeighbour\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_Location</th>\n",
       "      <th>id_Date</th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Climate</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Season</th>\n",
       "      <th>WindGustDir_cos</th>\n",
       "      <th>WindGustDir_sin</th>\n",
       "      <th>WindDir9am_cos</th>\n",
       "      <th>WindDir9am_sin</th>\n",
       "      <th>WindDir3pm_cos</th>\n",
       "      <th>WindDir3pm_sin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelaide</td>\n",
       "      <td>2008-07-01</td>\n",
       "      <td>2008-07-01</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>8.8</td>\n",
       "      <td>15.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>2008</td>\n",
       "      <td>July</td>\n",
       "      <td>Winter</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelaide</td>\n",
       "      <td>2008-07-02</td>\n",
       "      <td>2008-07-02</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>12.7</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>2008</td>\n",
       "      <td>July</td>\n",
       "      <td>Winter</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.923880</td>\n",
       "      <td>-0.382683</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelaide</td>\n",
       "      <td>2008-07-03</td>\n",
       "      <td>2008-07-03</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>6.2</td>\n",
       "      <td>15.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>2008</td>\n",
       "      <td>July</td>\n",
       "      <td>Winter</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.923880</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelaide</td>\n",
       "      <td>2008-07-04</td>\n",
       "      <td>2008-07-04</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>5.3</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>2008</td>\n",
       "      <td>July</td>\n",
       "      <td>Winter</td>\n",
       "      <td>9.238795e-01</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>0.923880</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelaide</td>\n",
       "      <td>2008-07-07</td>\n",
       "      <td>2008-07-07</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>7.6</td>\n",
       "      <td>11.2</td>\n",
       "      <td>16.2</td>\n",
       "      <td>46.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>2008</td>\n",
       "      <td>July</td>\n",
       "      <td>Winter</td>\n",
       "      <td>-3.826834e-01</td>\n",
       "      <td>-0.923880</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>-0.923880</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_Location     id_Date        Date  Location  MinTemp  MaxTemp  Rainfall  \\\n",
       "0    Adelaide  2008-07-01  2008-07-01  Adelaide      8.8     15.7       5.0   \n",
       "1    Adelaide  2008-07-02  2008-07-02  Adelaide     12.7     15.8       0.8   \n",
       "2    Adelaide  2008-07-03  2008-07-03  Adelaide      6.2     15.1       0.0   \n",
       "3    Adelaide  2008-07-04  2008-07-04  Adelaide      5.3     15.9       0.0   \n",
       "4    Adelaide  2008-07-07  2008-07-07  Adelaide      7.6     11.2      16.2   \n",
       "\n",
       "   WindGustSpeed  WindSpeed9am  WindSpeed3pm  ...    Climate  Year  Month  \\\n",
       "0           48.0          13.0          15.0  ...  Temperate  2008   July   \n",
       "1           35.0          13.0          15.0  ...  Temperate  2008   July   \n",
       "2           20.0           2.0          11.0  ...  Temperate  2008   July   \n",
       "3           30.0           6.0          13.0  ...  Temperate  2008   July   \n",
       "4           46.0          17.0          13.0  ...  Temperate  2008   July   \n",
       "\n",
       "   Season  WindGustDir_cos  WindGustDir_sin  WindDir9am_cos  WindDir9am_sin  \\\n",
       "0  Winter     7.071068e-01        -0.707107       -0.707107       -0.707107   \n",
       "1  Winter    -7.071068e-01        -0.707107       -0.923880       -0.382683   \n",
       "2  Winter    -1.836970e-16        -1.000000        0.923880        0.382683   \n",
       "3  Winter     9.238795e-01         0.382683        0.923880        0.382683   \n",
       "4  Winter    -3.826834e-01        -0.923880        0.382683       -0.923880   \n",
       "\n",
       "  WindDir3pm_cos  WindDir3pm_sin  \n",
       "0  -1.836970e-16       -1.000000  \n",
       "1  -7.071068e-01       -0.707107  \n",
       "2  -7.071068e-01       -0.707107  \n",
       "3   7.071068e-01        0.707107  \n",
       "4  -7.071068e-01       -0.707107  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data_saved/data_mat.csv')\n",
    "df_gps = pd.read_csv('../data/cities_coordinates_gps.csv', index_col = 0).T\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RainTomorrow\n",
       "0.0    0.778709\n",
       "1.0    0.221291\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['RainTomorrow'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 135600 entries, 0 to 135599\n",
      "Data columns (total 28 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   id_Location      135600 non-null  object \n",
      " 1   id_Date          135600 non-null  object \n",
      " 2   Date             135600 non-null  object \n",
      " 3   Location         135600 non-null  object \n",
      " 4   MinTemp          135600 non-null  float64\n",
      " 5   MaxTemp          135600 non-null  float64\n",
      " 6   Rainfall         135600 non-null  float64\n",
      " 7   WindGustSpeed    135600 non-null  float64\n",
      " 8   WindSpeed9am     135600 non-null  float64\n",
      " 9   WindSpeed3pm     135600 non-null  float64\n",
      " 10  Humidity9am      135600 non-null  float64\n",
      " 11  Humidity3pm      135600 non-null  float64\n",
      " 12  Pressure9am      135600 non-null  float64\n",
      " 13  Pressure3pm      135600 non-null  float64\n",
      " 14  Temp9am          135600 non-null  float64\n",
      " 15  Temp3pm          135600 non-null  float64\n",
      " 16  RainToday        135600 non-null  float64\n",
      " 17  RainTomorrow     135600 non-null  float64\n",
      " 18  Climate          135600 non-null  object \n",
      " 19  Year             135600 non-null  int64  \n",
      " 20  Month            135600 non-null  object \n",
      " 21  Season           135600 non-null  object \n",
      " 22  WindGustDir_cos  135600 non-null  float64\n",
      " 23  WindGustDir_sin  135600 non-null  float64\n",
      " 24  WindDir9am_cos   135600 non-null  float64\n",
      " 25  WindDir9am_sin   135600 non-null  float64\n",
      " 26  WindDir3pm_cos   135600 non-null  float64\n",
      " 27  WindDir3pm_sin   135600 non-null  float64\n",
      "dtypes: float64(20), int64(1), object(7)\n",
      "memory usage: 29.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encodage des locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gps_lat'] = df['Location'].map(lambda x : df_gps.loc[x, 'lat'])\n",
    "df['gps_lon'] = df['Location'].map(lambda x : df_gps.loc[x, 'long'])\n",
    "\n",
    "df['sin_lat'] = df['gps_lat'].apply(lambda x : np.sin(np.radians(x)))\n",
    "df['cos_lat'] = df['gps_lat'].apply(lambda x : np.cos(np.radians(x)))\n",
    "\n",
    "df['sin_lon'] = df['gps_lon'].apply(lambda x : np.round(np.sin(np.radians(x)), 6))\n",
    "df['cos_lon'] = df['gps_lon'].apply(lambda x : np.round(np.cos(np.radians(x)), 6))\n",
    "\n",
    "df = df.drop(columns = ['Location', 'id_Location', 'gps_lat', 'gps_lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encodage Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mois = {\n",
    "    'January': 1,\n",
    "    'February': 2,\n",
    "    'March': 3,\n",
    "    'April': 4,\n",
    "    'May': 5,\n",
    "    'June': 6,\n",
    "    'July': 7,\n",
    "    'August': 8,\n",
    "    'September': 9,\n",
    "    'October': 10,\n",
    "    'November': 11,\n",
    "    'December': 12\n",
    "}\n",
    "\n",
    "df['Month'] = df['Month'].map(dict_mois)\n",
    "\n",
    "df['sin_month'] = df['Month'].apply(lambda x : np.sin((2 * np.pi) * (( x - 1 ) / 12)))\n",
    "df['cos_month'] = df['Month'].apply(lambda x : np.cos((2 * np.pi) * (( x - 1 ) / 12)))\n",
    "\n",
    "\n",
    "df = df.drop(columns = 'Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encodage season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Winter', 'Spring', 'Summer', 'Autumn'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Season'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_127804/907855078.py:10: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df['sin_season'] = df['Season'].apply(lambda x : round(np.sin((2 * np.pi) * (x / 4))), 1)\n",
      "/tmp/ipykernel_127804/907855078.py:11: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  df['cos_season'] = df['Season'].apply(lambda x : round(np.cos((2 * np.pi) * ( x / 4))), 1)\n"
     ]
    }
   ],
   "source": [
    "dict_season = {\n",
    "    'Spring' : 0,\n",
    "    'Summer' : 1,\n",
    "    'Autumn' : 2,\n",
    "    'Winter' : 3\n",
    "}\n",
    "\n",
    "df['Season'] = df['Season'].map(dict_season)\n",
    "\n",
    "df['sin_season'] = df['Season'].apply(lambda x : round(np.sin((2 * np.pi) * (x / 4))), 1)\n",
    "df['cos_season'] = df['Season'].apply(lambda x : round(np.cos((2 * np.pi) * ( x / 4))), 1)\n",
    "\n",
    "df = df.drop(columns = 'Season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encodage Climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output = False)\n",
    "\n",
    "climate_transform = ohe.fit_transform(df[['Climate']])\n",
    "df_climate_ohe = pd.DataFrame(climate_transform, columns = ohe.get_feature_names_out())\n",
    "\n",
    "df = pd.concat([df, df_climate_ohe], axis = 1)\n",
    "\n",
    "df = df.drop(columns = 'Climate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_Date', 'Date', 'MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed',\n",
       "       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n",
       "       'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RainToday',\n",
       "       'RainTomorrow', 'Year', 'WindGustDir_cos', 'WindGustDir_sin',\n",
       "       'WindDir9am_cos', 'WindDir9am_sin', 'WindDir3pm_cos', 'WindDir3pm_sin',\n",
       "       'sin_lat', 'cos_lat', 'sin_lon', 'cos_lon', 'sin_month', 'cos_month',\n",
       "       'sin_season', 'cos_season', 'Climate_Desert', 'Climate_Grassland',\n",
       "       'Climate_Subtropical', 'Climate_Temperate', 'Climate_Tropical'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_Date</th>\n",
       "      <th>Date</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>cos_lon</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>sin_season</th>\n",
       "      <th>cos_season</th>\n",
       "      <th>Climate_Desert</th>\n",
       "      <th>Climate_Grassland</th>\n",
       "      <th>Climate_Subtropical</th>\n",
       "      <th>Climate_Temperate</th>\n",
       "      <th>Climate_Tropical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-07-01</td>\n",
       "      <td>2008-07-01</td>\n",
       "      <td>8.8</td>\n",
       "      <td>15.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.750119</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-07-02</td>\n",
       "      <td>2008-07-02</td>\n",
       "      <td>12.7</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.750119</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-07-03</td>\n",
       "      <td>2008-07-03</td>\n",
       "      <td>6.2</td>\n",
       "      <td>15.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.750119</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-07-04</td>\n",
       "      <td>2008-07-04</td>\n",
       "      <td>5.3</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.750119</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-07-07</td>\n",
       "      <td>2008-07-07</td>\n",
       "      <td>7.6</td>\n",
       "      <td>11.2</td>\n",
       "      <td>16.2</td>\n",
       "      <td>46.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.750119</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_Date        Date  MinTemp  MaxTemp  Rainfall  WindGustSpeed  \\\n",
       "0  2008-07-01  2008-07-01      8.8     15.7       5.0           48.0   \n",
       "1  2008-07-02  2008-07-02     12.7     15.8       0.8           35.0   \n",
       "2  2008-07-03  2008-07-03      6.2     15.1       0.0           20.0   \n",
       "3  2008-07-04  2008-07-04      5.3     15.9       0.0           30.0   \n",
       "4  2008-07-07  2008-07-07      7.6     11.2      16.2           46.0   \n",
       "\n",
       "   WindSpeed9am  WindSpeed3pm  Humidity9am  Humidity3pm  ...   cos_lon  \\\n",
       "0          13.0          15.0         92.0         67.0  ... -0.750119   \n",
       "1          13.0          15.0         75.0         52.0  ... -0.750119   \n",
       "2           2.0          11.0         81.0         56.0  ... -0.750119   \n",
       "3           6.0          13.0         71.0         46.0  ... -0.750119   \n",
       "4          17.0          13.0         83.0         88.0  ... -0.750119   \n",
       "\n",
       "      sin_month  cos_month  sin_season  cos_season  Climate_Desert  \\\n",
       "0  1.224647e-16       -1.0          -1           0             0.0   \n",
       "1  1.224647e-16       -1.0          -1           0             0.0   \n",
       "2  1.224647e-16       -1.0          -1           0             0.0   \n",
       "3  1.224647e-16       -1.0          -1           0             0.0   \n",
       "4  1.224647e-16       -1.0          -1           0             0.0   \n",
       "\n",
       "   Climate_Grassland  Climate_Subtropical  Climate_Temperate  Climate_Tropical  \n",
       "0                0.0                  0.0                1.0               0.0  \n",
       "1                0.0                  0.0                1.0               0.0  \n",
       "2                0.0                  0.0                1.0               0.0  \n",
       "3                0.0                  0.0                1.0               0.0  \n",
       "4                0.0                  0.0                1.0               0.0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['id_Date', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - modelisation simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'RainTomorrow')\n",
    "y = df['RainTomorrow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a - tres simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.8478982300884956\n",
      "\n",
      "f1 score :  0.5855520948457752\n",
      "roc-auc score :  0.7194746831985506\n",
      "[[20081  1106]\n",
      " [ 3019  2914]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.95      0.91     21187\n",
      "         1.0       0.72      0.49      0.59      5933\n",
      "\n",
      "    accuracy                           0.85     27120\n",
      "   macro avg       0.80      0.72      0.75     27120\n",
      "weighted avg       0.84      0.85      0.84     27120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathieu/.pyenv/versions/3.10.6/envs/meteo-venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b - tres simple avec stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.8476401179941003\n",
      "\n",
      "f1 score :  0.5931469082315872\n",
      "roc-auc score :  0.7238972332522922\n",
      "[[19976  1143]\n",
      " [ 2989  3012]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.95      0.91     21119\n",
      "         1.0       0.72      0.50      0.59      6001\n",
      "\n",
      "    accuracy                           0.85     27120\n",
      "   macro avg       0.80      0.72      0.75     27120\n",
      "weighted avg       0.84      0.85      0.84     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c - avec mise à l'echelle [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.846976401179941\n",
      "\n",
      "f1 score :  0.5917765099350777\n",
      "roc-auc score :  0.7232325003774212\n",
      "[[19962  1157]\n",
      " [ 2993  3008]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.95      0.91     21119\n",
      "         1.0       0.72      0.50      0.59      6001\n",
      "\n",
      "    accuracy                           0.85     27120\n",
      "   macro avg       0.80      0.72      0.75     27120\n",
      "weighted avg       0.84      0.85      0.84     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d - avec standardscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.8474926253687316\n",
      "\n",
      "f1 score :  0.5929935052155088\n",
      "roc-auc score :  0.723862175881967\n",
      "[[19971  1148]\n",
      " [ 2988  3013]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.95      0.91     21119\n",
      "         1.0       0.72      0.50      0.59      6001\n",
      "\n",
      "    accuracy                           0.85     27120\n",
      "   macro avg       0.80      0.72      0.75     27120\n",
      "weighted avg       0.84      0.85      0.84     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.8471976401179941\n",
      "\n",
      "f1 score :  0.5921259842519685\n",
      "roc-auc score :  0.723374552557922\n",
      "[[19968  1151]\n",
      " [ 2993  3008]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.95      0.91     21119\n",
      "         1.0       0.72      0.50      0.59      6001\n",
      "\n",
      "    accuracy                           0.85     27120\n",
      "   macro avg       0.80      0.72      0.75     27120\n",
      "weighted avg       0.84      0.85      0.84     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion 1 : je continue avec un minmaxscaler [-1, 1], augmenter le nom d'iter ne sert à rien, je garde straify = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Re *Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a - undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 16:14:45.058979: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-17 16:14:45.341454: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-17 16:14:45.341531: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-17 16:14:45.343004: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-17 16:14:45.491259: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-17 16:14:45.493451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.7934734513274336\n",
      "\n",
      "f1 score :  0.6235634115195914\n",
      "roc-auc score :  0.7861590479904785\n",
      "[[16880  4239]\n",
      " [ 1362  4639]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.80      0.86     21119\n",
      "         1.0       0.52      0.77      0.62      6001\n",
      "\n",
      "    accuracy                           0.79     27120\n",
      "   macro avg       0.72      0.79      0.74     27120\n",
      "weighted avg       0.84      0.79      0.81     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "sample = RandomUnderSampler(random_state = 42)\n",
    "\n",
    "X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "\n",
    "model.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b - oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.7923672566371681\n",
      "\n",
      "f1 score :  0.621496269409155\n",
      "roc-auc score :  0.784494481754501\n",
      "[[16866  4253]\n",
      " [ 1378  4623]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.80      0.86     21119\n",
      "         1.0       0.52      0.77      0.62      6001\n",
      "\n",
      "    accuracy                           0.79     27120\n",
      "   macro avg       0.72      0.78      0.74     27120\n",
      "weighted avg       0.84      0.79      0.80     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "sample = SMOTE(random_state = 42)\n",
    "\n",
    "X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "\n",
    "model.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion : les stratégie d'undersampling est la plus efficace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - test différence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.7932890855457227\n",
      "\n",
      "f1 score :  0.6232020432853879\n",
      "roc-auc score :  0.7858617389233682\n",
      "[[16878  4241]\n",
      " [ 1365  4636]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.80      0.86     21119\n",
      "         1.0       0.52      0.77      0.62      6001\n",
      "\n",
      "    accuracy                           0.79     27120\n",
      "   macro avg       0.72      0.79      0.74     27120\n",
      "weighted avg       0.84      0.79      0.81     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "sample = RandomUnderSampler(random_state = 42)\n",
    "\n",
    "X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegressionCV(max_iter = 500)\n",
    "\n",
    "model.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.782853982300885\n",
      "\n",
      "f1 score :  0.6125912768896783\n",
      "roc-auc score :  0.7803544927432466\n",
      "[[16575  4544]\n",
      " [ 1345  4656]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.78      0.85     21119\n",
      "         1.0       0.51      0.78      0.61      6001\n",
      "\n",
      "    accuracy                           0.78     27120\n",
      "   macro avg       0.72      0.78      0.73     27120\n",
      "weighted avg       0.83      0.78      0.80     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "sample = RandomUnderSampler(random_state = 42)\n",
    "\n",
    "X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = SGDClassifier(max_iter = 2000, n_jobs = -1)\n",
    "\n",
    "model.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.8164454277286136\n",
      "\n",
      "f1 score :  0.6619125237707145\n",
      "roc-auc score :  0.8148655149012011\n",
      "[[17269  3850]\n",
      " [ 1128  4873]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.82      0.87     21119\n",
      "         1.0       0.56      0.81      0.66      6001\n",
      "\n",
      "    accuracy                           0.82     27120\n",
      "   macro avg       0.75      0.81      0.77     27120\n",
      "weighted avg       0.85      0.82      0.83     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "sample = RandomUnderSampler(random_state = 42)\n",
    "\n",
    "X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "model.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.7223451327433629\n",
      "\n",
      "f1 score :  0.5371850030731408\n",
      "roc-auc score :  0.7244450135404064\n",
      "[[15220  5899]\n",
      " [ 1631  4370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.72      0.80     21119\n",
      "         1.0       0.43      0.73      0.54      6001\n",
      "\n",
      "    accuracy                           0.72     27120\n",
      "   macro avg       0.66      0.72      0.67     27120\n",
      "weighted avg       0.80      0.72      0.74     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "sample = RandomUnderSampler(random_state = 42)\n",
    "\n",
    "X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "model.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]WARN: libsvm Solver reached max_iter\n",
      "optimization finished, #iter = 500\n",
      "obj = -985.535335, rho = -0.493495\n",
      "nSV = 1000, nBSV = 1000\n",
      "Total nSV = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathieu/.pyenv/versions/3.10.6/envs/meteo-venv/lib/python3.10/site-packages/sklearn/svm/_base.py:304: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.7803466076696165\n",
      "\n",
      "f1 score :  0.3735408560311284\n",
      "roc-auc score :  0.6069696080058125\n",
      "[[19387  1732]\n",
      " [ 4225  1776]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.92      0.87     21119\n",
      "         1.0       0.51      0.30      0.37      6001\n",
      "\n",
      "    accuracy                           0.78     27120\n",
      "   macro avg       0.66      0.61      0.62     27120\n",
      "weighted avg       0.75      0.78      0.76     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "sample = RandomUnderSampler(random_state = 42)\n",
    "\n",
    "X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = SVC(verbose = 2, max_iter = 500)\n",
    "\n",
    "model.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion : on va partir sur un RandomForrestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - exploration des hyperparametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.8692109144542773\n",
      "\n",
      "f1 score :  0.671175762068007\n",
      "roc-auc score :  0.7740828609629506\n",
      "[[19949  1170]\n",
      " [ 2379  3622]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.92     21119\n",
      "         1.0       0.76      0.60      0.67      6001\n",
      "\n",
      "    accuracy                           0.87     27120\n",
      "   macro avg       0.82      0.77      0.79     27120\n",
      "weighted avg       0.86      0.87      0.86     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_s, y_train_s = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators = 200,\n",
    "    max_features = 'sqrt',\n",
    "    criterion = 'entropy',\n",
    "    max_depth = 30,\n",
    "    bootstrap = False,\n",
    "    min_samples_split = 5,\n",
    "    min_samples_leaf = 2,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score accuracy :  0.8155604719764011\n",
      "\n",
      "f1 score :  0.6604670105891935\n",
      "roc-auc score :  0.8138201535124608\n",
      "[[17253  3866]\n",
      " [ 1136  4865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.82      0.87     21119\n",
      "         1.0       0.56      0.81      0.66      6001\n",
      "\n",
      "    accuracy                           0.82     27120\n",
      "   macro avg       0.75      0.81      0.77     27120\n",
      "weighted avg       0.85      0.82      0.83     27120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "sample = RandomUnderSampler(random_state = 42)\n",
    "\n",
    "X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "model.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print('score accuracy : ', model.score(X_test_scaled, y_test), end = '\\n\\n')\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres : {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Meilleur score F1 : 0.8124270767458042\n",
      "f1 score :  0.6672975530620522\n",
      "roc-auc score :  0.8199489125030924\n",
      "[[17262  3857]\n",
      " [ 1065  4936]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.82      0.88     21119\n",
      "         1.0       0.56      0.82      0.67      6001\n",
      "\n",
      "    accuracy                           0.82     27120\n",
      "   macro avg       0.75      0.82      0.77     27120\n",
      "weighted avg       0.86      0.82      0.83     27120\n",
      "\n",
      "Score accuracy :  0.818547197640118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "sample = RandomUnderSampler(random_state=42)\n",
    "X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_features': ['sqrt'],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [30],\n",
    "    'bootstrap': [False],\n",
    "    'min_samples_split': [5],\n",
    "    'min_samples_leaf': [2],\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print(f\"Meilleurs paramètres : {grid_search.best_params_}\")\n",
    "print(f\"Meilleur score F1 : {grid_search.best_score_}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('Score accuracy : ', best_model.score(X_test_scaled, y_test), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres : {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Meilleur score F1 : 0.8110884959704208\n",
      "f1 score :  0.6655824354543607\n",
      "roc-auc score :  0.8181500306951225\n",
      "[[17274  3845]\n",
      " [ 1090  4911]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.82      0.88     21119\n",
      "         1.0       0.56      0.82      0.67      6001\n",
      "\n",
      "    accuracy                           0.82     27120\n",
      "   macro avg       0.75      0.82      0.77     27120\n",
      "weighted avg       0.86      0.82      0.83     27120\n",
      "\n",
      "Score accuracy :  0.8180309734513275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "sample = RandomUnderSampler(random_state=42)\n",
    "X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_features': ['sqrt'],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [30],\n",
    "    'bootstrap': [False],\n",
    "    'min_samples_split': [5],\n",
    "    'min_samples_leaf': [2],\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print(f\"Meilleurs paramètres : {grid_search.best_params_}\")\n",
    "print(f\"Meilleur score F1 : {grid_search.best_score_}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('Score accuracy : ', best_model.score(X_test_scaled, y_test), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres : {'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt'}\n",
      "Meilleur score F1 : 0.8111800845412434\n",
      "f1 score :  0.6603183240375459\n",
      "roc-auc score :  0.813353471503033\n",
      "[[17272  3847]\n",
      " [ 1147  4854]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.82      0.87     21119\n",
      "         1.0       0.56      0.81      0.66      6001\n",
      "\n",
      "    accuracy                           0.82     27120\n",
      "   macro avg       0.75      0.81      0.77     27120\n",
      "weighted avg       0.85      0.82      0.83     27120\n",
      "\n",
      "Score accuracy :  0.8158554572271386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "sample = RandomUnderSampler(random_state=42)\n",
    "X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grid = {\n",
    "    # 'n_estimators': [100, 200],\n",
    "    'max_features': ['sqrt'],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    # 'min_samples_split': [2, 5, 10],\n",
    "    # 'min_samples_leaf': [1, 2, 4],\n",
    "    # 'bootstrap': [True, False],\n",
    "    # 'warm_start': [True, False]\n",
    "}\n",
    "\n",
    "model = ExtraTreesClassifier(\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print(f\"Meilleurs paramètres : {grid_search.best_params_}\")\n",
    "print(f\"Meilleur score F1 : {grid_search.best_score_}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "print('f1 score : ', f1_score(y_test, y_pred))\n",
    "print('roc-auc score : ', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('Score accuracy : ', best_model.score(X_test_scaled, y_test), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathieu/.pyenv/versions/3.10.6/envs/meteo-venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.6957 - loss: 0.2013 - precision_31: 0.8532 - val_accuracy: 0.8472 - val_loss: 0.3523 - val_precision_31: 0.7137 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7274 - loss: 0.1774 - precision_31: 0.8969 - val_accuracy: 0.8522 - val_loss: 0.3432 - val_precision_31: 0.7280 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7299 - loss: 0.1745 - precision_31: 0.9012 - val_accuracy: 0.8555 - val_loss: 0.3376 - val_precision_31: 0.7387 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7331 - loss: 0.1730 - precision_31: 0.9027 - val_accuracy: 0.8542 - val_loss: 0.3390 - val_precision_31: 0.7160 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7386 - loss: 0.1699 - precision_31: 0.9056 - val_accuracy: 0.8563 - val_loss: 0.3356 - val_precision_31: 0.7429 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7451 - loss: 0.1669 - precision_31: 0.9109 - val_accuracy: 0.8564 - val_loss: 0.3350 - val_precision_31: 0.7274 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7411 - loss: 0.1680 - precision_31: 0.9131 - val_accuracy: 0.8554 - val_loss: 0.3343 - val_precision_31: 0.7359 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7432 - loss: 0.1663 - precision_31: 0.9083 - val_accuracy: 0.8571 - val_loss: 0.3306 - val_precision_31: 0.7422 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7445 - loss: 0.1667 - precision_31: 0.9125 - val_accuracy: 0.8575 - val_loss: 0.3322 - val_precision_31: 0.7244 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7460 - loss: 0.1656 - precision_31: 0.9072 - val_accuracy: 0.8591 - val_loss: 0.3287 - val_precision_31: 0.7575 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7467 - loss: 0.1659 - precision_31: 0.9104 - val_accuracy: 0.8584 - val_loss: 0.3280 - val_precision_31: 0.7376 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7450 - loss: 0.1668 - precision_31: 0.9136 - val_accuracy: 0.8593 - val_loss: 0.3296 - val_precision_31: 0.7488 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7408 - loss: 0.1671 - precision_31: 0.9077 - val_accuracy: 0.8593 - val_loss: 0.3290 - val_precision_31: 0.7490 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7521 - loss: 0.1626 - precision_31: 0.9118 - val_accuracy: 0.8589 - val_loss: 0.3317 - val_precision_31: 0.7717 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7481 - loss: 0.1632 - precision_31: 0.9147 - val_accuracy: 0.8591 - val_loss: 0.3304 - val_precision_31: 0.7446 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7431 - loss: 0.1664 - precision_31: 0.9104 - val_accuracy: 0.8586 - val_loss: 0.3275 - val_precision_31: 0.7714 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7499 - loss: 0.1626 - precision_31: 0.9132 - val_accuracy: 0.8589 - val_loss: 0.3261 - val_precision_31: 0.7461 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7483 - loss: 0.1637 - precision_31: 0.9103 - val_accuracy: 0.8588 - val_loss: 0.3267 - val_precision_31: 0.7583 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7468 - loss: 0.1648 - precision_31: 0.9135 - val_accuracy: 0.8601 - val_loss: 0.3282 - val_precision_31: 0.7410 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7448 - loss: 0.1645 - precision_31: 0.9140 - val_accuracy: 0.8588 - val_loss: 0.3276 - val_precision_31: 0.7658 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7515 - loss: 0.1614 - precision_31: 0.9174 - val_accuracy: 0.8586 - val_loss: 0.3276 - val_precision_31: 0.7373 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7472 - loss: 0.1619 - precision_31: 0.9135 - val_accuracy: 0.8612 - val_loss: 0.3244 - val_precision_31: 0.7534 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7538 - loss: 0.1617 - precision_31: 0.9175 - val_accuracy: 0.8603 - val_loss: 0.3244 - val_precision_31: 0.7594 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7475 - loss: 0.1629 - precision_31: 0.9152 - val_accuracy: 0.8605 - val_loss: 0.3250 - val_precision_31: 0.7357 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7535 - loss: 0.1609 - precision_31: 0.9148 - val_accuracy: 0.8608 - val_loss: 0.3253 - val_precision_31: 0.7295 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7502 - loss: 0.1617 - precision_31: 0.9106 - val_accuracy: 0.8594 - val_loss: 0.3253 - val_precision_31: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7512 - loss: 0.1617 - precision_31: 0.9146 - val_accuracy: 0.8598 - val_loss: 0.3240 - val_precision_31: 0.7436 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7506 - loss: 0.1614 - precision_31: 0.9125 - val_accuracy: 0.8595 - val_loss: 0.3254 - val_precision_31: 0.7425 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7523 - loss: 0.1621 - precision_31: 0.9151 - val_accuracy: 0.8608 - val_loss: 0.3237 - val_precision_31: 0.7628 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7482 - loss: 0.1621 - precision_31: 0.9136 - val_accuracy: 0.8594 - val_loss: 0.3246 - val_precision_31: 0.7533 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7503 - loss: 0.1624 - precision_31: 0.9167 - val_accuracy: 0.8598 - val_loss: 0.3239 - val_precision_31: 0.7387 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.1607 - precision_31: 0.9168 - val_accuracy: 0.8614 - val_loss: 0.3235 - val_precision_31: 0.7508 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7505 - loss: 0.1612 - precision_31: 0.9167 - val_accuracy: 0.8610 - val_loss: 0.3230 - val_precision_31: 0.7467 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.1606 - precision_31: 0.9168 - val_accuracy: 0.8608 - val_loss: 0.3229 - val_precision_31: 0.7564 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 21ms/step - accuracy: 0.7559 - loss: 0.1594 - precision_31: 0.9166 - val_accuracy: 0.8617 - val_loss: 0.3231 - val_precision_31: 0.7567 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.7507 - loss: 0.1607 - precision_31: 0.9177 - val_accuracy: 0.8609 - val_loss: 0.3230 - val_precision_31: 0.7525 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.1594 - precision_31: 0.9143 - val_accuracy: 0.8598 - val_loss: 0.3250 - val_precision_31: 0.7617 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m2975/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7501 - loss: 0.1607 - precision_31: 0.9168\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7501 - loss: 0.1607 - precision_31: 0.9168 - val_accuracy: 0.8605 - val_loss: 0.3229 - val_precision_31: 0.7608 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7526 - loss: 0.1595 - precision_31: 0.9203 - val_accuracy: 0.8606 - val_loss: 0.3223 - val_precision_31: 0.7397 - learning_rate: 1.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.1598 - precision_31: 0.9159 - val_accuracy: 0.8607 - val_loss: 0.3223 - val_precision_31: 0.7420 - learning_rate: 1.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7582 - loss: 0.1579 - precision_31: 0.9200 - val_accuracy: 0.8617 - val_loss: 0.3216 - val_precision_31: 0.7481 - learning_rate: 1.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7589 - loss: 0.1567 - precision_31: 0.9228 - val_accuracy: 0.8617 - val_loss: 0.3214 - val_precision_31: 0.7469 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7549 - loss: 0.1589 - precision_31: 0.9202 - val_accuracy: 0.8616 - val_loss: 0.3216 - val_precision_31: 0.7473 - learning_rate: 1.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7549 - loss: 0.1571 - precision_31: 0.9216 - val_accuracy: 0.8618 - val_loss: 0.3213 - val_precision_31: 0.7463 - learning_rate: 1.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7545 - loss: 0.1584 - precision_31: 0.9167 - val_accuracy: 0.8612 - val_loss: 0.3218 - val_precision_31: 0.7442 - learning_rate: 1.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7602 - loss: 0.1565 - precision_31: 0.9206 - val_accuracy: 0.8619 - val_loss: 0.3211 - val_precision_31: 0.7483 - learning_rate: 1.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7524 - loss: 0.1599 - precision_31: 0.9199 - val_accuracy: 0.8617 - val_loss: 0.3222 - val_precision_31: 0.7494 - learning_rate: 1.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7597 - loss: 0.1583 - precision_31: 0.9201 - val_accuracy: 0.8620 - val_loss: 0.3212 - val_precision_31: 0.7486 - learning_rate: 1.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7555 - loss: 0.1579 - precision_31: 0.9214 - val_accuracy: 0.8615 - val_loss: 0.3217 - val_precision_31: 0.7387 - learning_rate: 1.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m3001/3001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7551 - loss: 0.1597 - precision_31: 0.9143 - val_accuracy: 0.8613 - val_loss: 0.3215 - val_precision_31: 0.7444 - learning_rate: 1.0000e-04\n",
      "\u001b[1m848/848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 814us/step\n",
      "f1 score :  0.6446187281489181\n",
      "roc-auc score :  0.7564787823334115\n",
      "[[19948  1171]\n",
      " [ 2590  3411]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.91     21119\n",
      "         1.0       0.74      0.57      0.64      6001\n",
      "\n",
      "    accuracy                           0.86     27120\n",
      "   macro avg       0.81      0.76      0.78     27120\n",
      "weighted avg       0.85      0.86      0.85     27120\n",
      "\n",
      "\u001b[1m848/848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8640 - loss: 0.3191 - precision_31: 0.7447\n",
      "Score accuracy :  0.8613200783729553\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# sample = RandomUnderSampler(random_state=42)\n",
    "# X_train_s, y_train_s = sample.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_s)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.1,\n",
    "                              patience = 5,\n",
    "                              min_lr = 1e-6,\n",
    "                              verbose = 1)\n",
    "\n",
    "early_stp = EarlyStopping(monitor = 'val_accuracy',\n",
    "                          patience = 5)\n",
    "\n",
    "model_dense = Sequential()\n",
    "model_dense.add(Dense(36, input_dim=X_train_scaled.shape[1], activation='tanh'))\n",
    "model_dense.add(Dropout(0.2))\n",
    "model_dense.add(Dense(36, input_dim=X_train_scaled.shape[1], activation='tanh'))\n",
    "model_dense.add(Dropout(0.2))\n",
    "model_dense.add(Dense(36, input_dim=X_train_scaled.shape[1], activation='tanh'))\n",
    "model_dense.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_dense.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy', Precision()])\n",
    "\n",
    "class_weight_dict = {0: 0.77, 1: 0.22}\n",
    "\n",
    "history_dense = model_dense.fit(X_train_scaled, y_train_s,\n",
    "                                epochs = 50,\n",
    "                                batch_size = 16,\n",
    "                                validation_data=(X_test_scaled, y_test),\n",
    "                                verbose = 1,\n",
    "                                callbacks = [reduce_lr],\n",
    "                                class_weight=class_weight_dict)\n",
    "\n",
    "y_pred_dense = (model_dense.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"f1 score : \", f1_score(y_test, y_pred_dense))\n",
    "print(\"roc-auc score : \", roc_auc_score(y_test, y_pred_dense))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_dense))\n",
    "print(classification_report(y_test, y_pred_dense))\n",
    "\n",
    "print(\"Score accuracy : \", model_dense.evaluate(X_test_scaled, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meteo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
